{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[주의] \n",
    "!!!! 아직 dropout_data 만드는 중! 어디까지 만들었는지 확인하고 코드 돌리세요 존재 안하는 폴더 있을수도 !!!!\n",
    "!!!!!dropout_data의 X는 sparse array, int32으로 통일했습니다.!!!!!!\n",
    "!!!! adata 모두 normalize/log 했습니다. 전에 없던 오류가 생겼으면 얘가 원인일수도... !!!\n",
    "봐서 알겠지만 원본/dropout 파일들의 이름이 같습니다. 코딩하기 편하려고 통일했으니\n",
    "아무쪼록 헷갈리지 마시길...\n",
    "dropout_data에 layers/dropped가 추가되어있습니다. X랑 같은 모양으로 boolean \n",
    "형식이고, True인 데이터들이 drop 후 0으로 채운 위치입니다. 혹시 모델이 NaN을\n",
    "더 잘 impute한다면 이거 이용해서 데이터 수정하시면 됩니다.\n",
    "모델 별로 이 노트북을 새로 복제해서 만들어두면 나중에 배포용 코드 만들기 편하겠죠?\n",
    "이미지 파일이 필요하시면 hest_data/wsis 에서 가져오셔야 합니다. SPROD에 제가 만들어둔 가져오는\n",
    "코드가 있긴 하니 필요하시면 가져가시죠.\n",
    "이왕이면 dropout_data 폴더는 수정하지 말아주세요 (output 저장 포함)\n",
    "\n",
    "결과는 bench_results에 저장됩니다. 혹시 잘못 돌려서 데이터 삭제해야하면 수동으로 들어가서 수정하시죠 ㅎ\n",
    "\n",
    "[수정] 이라고 코멘트 해 둔 부분 적절히 고치시면 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패키지 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "\n",
    "\n",
    "#[수정] 이 밑으로 [여기까지] 지우고 여러분 모델에 맞는 패키지 불러오시면 됩니다.\n",
    "import pandas as pd\n",
    "from tifffile import TiffFile, imwrite\n",
    "import shutil\n",
    "#[여기까지]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벤치마킹 데이터/환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_data_dir = '/home/s2022310812/bench_data' # bench_data 경로 [수정]\n",
    "dropout_data_dir = '/home/s2022310812/dropout_data' #dropout_data 경로 [수정]\n",
    "datasets = [ #[수정]\n",
    "    \"IDC\",\n",
    "  # \"COAD\",\n",
    "  # \"READ\",\n",
    "  # \"LYMPH_IDC\",\n",
    "]\n",
    "model = \"SPROD\" #[수정] 모델명\n",
    "percents = [ #[수정] dropout percentage\n",
    "    # '10%',\n",
    "    #'20%',\n",
    "    '30%'\n",
    "] \n",
    "samples = [ #[수정] dropout percentage data version\n",
    "    '1',\n",
    "    # '2',\n",
    "    #'3',\n",
    "    #'4',\n",
    "    #'5'\n",
    "] \n",
    "\n",
    "trial = '1' #[수정] dropout percentage data version trial number (1-5)\n",
    "man_sc = {}\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu') #[수정] cuda:사용할 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벤치마킹 모델 구동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI785.h5ad/conv/NCBI785.tif already exists. Skipping copy.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI785.h5ad/conv/Counts.txt already exists. Skipping creation.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI785.h5ad/conv/Spot_metadata.csv already exists. Skipping creation.\n",
      "{'IDC/NCBI785.h5ad': 1.0926849639570606}\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI783.h5ad/conv/NCBI783.tif already exists. Skipping copy.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI783.h5ad/conv/Counts.txt already exists. Skipping creation.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/NCBI783.h5ad/conv/Spot_metadata.csv already exists. Skipping creation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IDC/NCBI785.h5ad': 1.0926849639570606, 'IDC/NCBI783.h5ad': 1.3124477419668465}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/SPROD/bench_test/30%-1/TENX95.h5ad/conv/TENX95.tif already exists. Skipping copy.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/TENX95.h5ad/conv/Counts.txt already exists. Skipping creation.\n",
      "/home/s2022310812/SPROD/bench_test/30%-1/TENX95.h5ad/conv/Spot_metadata.csv already exists. Skipping creation.\n",
      "{'IDC/NCBI785.h5ad': 1.0926849639570606, 'IDC/NCBI783.h5ad': 1.3124477419668465, 'IDC/TENX95.h5ad': 1.4379274056360023}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/s2022310812/.conda/envs/sprod2/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "wsis_dir = '/home/s2022310812/hest_data/wsis'\n",
    "\n",
    "for dataset in datasets:\n",
    "    for percent in percents:\n",
    "        for sample in samples:\n",
    "            bench_path = os.path.join(bench_data_dir,dataset,'adata')\n",
    "            dropout_path = os.path.join(dropout_data_dir,dataset,percent+\"-\"+sample)\n",
    "            \n",
    "            adata_names = [f for f in os.listdir(bench_path) if os.path.isfile(os.path.join(bench_path, f))]\n",
    "        \n",
    "            for adata_name in adata_names:\n",
    "\n",
    "                adata = ad.read_h5ad(os.path.join(bench_path,adata_name))\n",
    "                sc.pp.normalize_total(adata, target_sum=1e6, exclude_highly_expressed= False) \n",
    "                sc.pp.log1p(adata)\n",
    "                sc.pp.scale(adata)\n",
    "                \n",
    "                adata_drop = ad.read_h5ad(os.path.join(dropout_path,adata_name))\n",
    "                sc.pp.normalize_total(adata_drop, target_sum=1e6, exclude_highly_expressed= False)\n",
    "                sc.pp.log1p(adata_drop)\n",
    "                sc.pp.scale(adata_drop)\n",
    "                dropped_mask = torch.tensor(adata_drop.layers['dropped'], dtype=torch.bool, device=device)\n",
    "                \n",
    "                \"\"\"\n",
    "                [수정]\n",
    "                여기서부터 [여기까지] adata_drop.X에 본인 모델 적용시켜서 adata2에 저장해주시면 됩니다.\n",
    "                [주의] adata2에 \"\"adata.X 에 해당하는 파일\"\" 저장해주세요... 그냥 그렇게 만들었어요..\n",
    "                예시로 제 코드 보여드림. 여러분한테는 쓸모가 없겠죠? 지우고 시작하시면 됩니다 화이팅\n",
    "                \"\"\"\n",
    "                # Define the custom directory paths\n",
    "                convert_dir = os.path.join(\"/home/s2022310812/SPROD/bench_test\",percent+\"-\"+sample, adata_name, 'conv')\n",
    "                out_dir = os.path.join(\"/home/s2022310812/SPROD/bench_test\",percent+\"-\"+sample, adata_name, 'out')\n",
    "        \n",
    "                # Ensure the directories exist\n",
    "                os.makedirs(convert_dir, exist_ok=True)\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "                # Remove the .h5ad extension from adata_name to get the base name (e.g., \"aaa\")\n",
    "                base_name = adata_name.replace(\".h5ad\", \"\")\n",
    "        \n",
    "                # Define the path for the .tif file and the convert_dir\n",
    "                tif_file = os.path.join(wsis_dir, f\"{base_name}.tif\")\n",
    "                tif_target_file = os.path.join(convert_dir, f\"{base_name}.tif\")\n",
    "        \n",
    "                # Check if the .tif file exists and copy it to the convert_dir if it does\n",
    "        \n",
    "                def copy_highest_resolution_tif(tif_file, tif_target_file):\n",
    "                    if os.path.exists(tif_file):\n",
    "                        if not os.path.exists(tif_target_file):  # Check if the .tif file is already in the target directory\n",
    "                            try:\n",
    "                                with TiffFile(tif_file) as tif:\n",
    "                                    # Extract the highest resolution (first series/page)\n",
    "                                    highest_res_image = tif.pages[0].asarray()\n",
    "                                    print(tif.pages[0].asarray().shape)\n",
    "        \n",
    "                                    # Save the highest resolution to the target location\n",
    "                                    imwrite(tif_target_file, highest_res_image)\n",
    "                                    print(f\"Copied highest resolution of {tif_file}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {tif_file}: {e}\")\n",
    "                        else:\n",
    "                            print(f\"{tif_target_file} already exists. Skipping copy.\")\n",
    "                    else:\n",
    "                        print(f\"File {tif_file} does not exist.\")\n",
    "                if not base_name == \"TENX99\": \n",
    "                    copy_highest_resolution_tif(tif_file, tif_target_file)\n",
    "                if base_name == \"TENX99\":\n",
    "                    continue\n",
    "                # Check if adata_drop.X is sparse or dense, and convert accordingly\n",
    "                if isinstance(adata_drop.X, csr_matrix):  # Sparse matrix\n",
    "                    expression_matrix = pd.DataFrame(adata_drop.X.toarray(), index=adata_drop.obs.index, columns=adata_drop.var.index)\n",
    "                    is_sparse = True\n",
    "                else:  # Dense matrix (numpy ndarray)\n",
    "                    expression_matrix = pd.DataFrame(adata_drop.X, index=adata_drop.obs.index, columns=adata_drop.var.index)\n",
    "                    is_sparse = False\n",
    "        \n",
    "                expression_matrix.index.name = \"\"  # Remove index name for cleaner output\n",
    "        \n",
    "                # Extract spatial coordinates for Spot_metadata.csv\n",
    "                spatial_coords = pd.DataFrame(adata_drop.obsm[\"spatial\"], columns=[\"X\", \"Y\"])\n",
    "                spatial_coords[\"X\"], spatial_coords[\"Y\"] = spatial_coords[\"Y\"].copy(), spatial_coords[\"X\"].copy()\n",
    "                spatial_coords.index = adata_drop.obs.index  # Match indices with the expression matrix\n",
    "        \n",
    "                # Calculate the horizontal and vertical distances between the first two spots\n",
    "                x_diff = abs(adata_drop.obs['pxl_col_in_fullres'].iloc[1] - adata_drop.obs['pxl_col_in_fullres'].iloc[0])\n",
    "                y_diff = abs(adata_drop.obs['pxl_row_in_fullres'].iloc[1] - adata_drop.obs['pxl_row_in_fullres'].iloc[0])\n",
    "        \n",
    "                # Estimate the spot radius using the maximum of the two distances\n",
    "                spot_radius = max(x_diff, y_diff) / 2  # Use the maximum of the two distances as the radius\n",
    "        \n",
    "                # Set the Spot_radius for each entry (you can apply this value uniformly or adjust as needed)\n",
    "                spatial_coords[\"Spot_radius\"] = spot_radius\n",
    "        \n",
    "                # Check if the files already exist and only create them if they don't\n",
    "                counts_file = os.path.join(convert_dir, \"Counts.txt\")\n",
    "                spot_metadata_file = os.path.join(convert_dir, \"Spot_metadata.csv\")\n",
    "        \n",
    "                if not os.path.exists(counts_file):\n",
    "                    expression_matrix.to_csv(counts_file, sep=\"\\t\")\n",
    "                    print(f\"Created {counts_file}\")\n",
    "                else:\n",
    "                    print(f\"{counts_file} already exists. Skipping creation.\")\n",
    "        \n",
    "                if not os.path.exists(spot_metadata_file):\n",
    "                    spatial_coords.to_csv(spot_metadata_file, index=True, index_label=\"\", header=True)\n",
    "                    print(f\"Created {spot_metadata_file}\")\n",
    "                else:\n",
    "                    print(f\"{spot_metadata_file} already exists. Skipping creation.\")\n",
    "        \n",
    "                denoised_matrix_file = os.path.join(out_dir, 'sprod_Denoised_matrix.txt')  # Ensure the correct file name\n",
    "                if not os.path.exists(denoised_matrix_file):\n",
    "                    denoised_matrix_file = os.path.join(out_dir, 'denoised_stitched.txt')  # Ensure the correct file name\n",
    "                \n",
    "        \n",
    "                # Run the sprod.py script\n",
    "                if not os.path.exists(denoised_matrix_file):\n",
    "                    if base_name ==\"NCBI783\" or base_name==\"NCBI785\":\n",
    "                        !python /home/s2022310812/SPROD/sprod.py {convert_dir} {out_dir} -y \"single\"\n",
    "                    else:\n",
    "                        !python /home/s2022310812/SPROD/sprod.py {convert_dir} {out_dir} -y \"batch\"\n",
    "                \n",
    "                denoised_matrix_file = os.path.join(out_dir, 'sprod_Denoised_matrix.txt')  # Ensure the correct file name\n",
    "                if not os.path.exists(denoised_matrix_file):\n",
    "                    denoised_matrix_file = os.path.join(out_dir, 'denoised_stitched.txt')  # Ensure the correct file name\n",
    "                \n",
    "        \n",
    "                # Load the output denoised matrix\n",
    "                counts = pd.read_csv(denoised_matrix_file, sep=\"\\t\", index_col=0)\n",
    "        \n",
    "                # Store the denoised matrix in adata2\n",
    "                if is_sparse:\n",
    "                    adata2 = csr_matrix(counts.values)  # Convert back to sparse matrix (csr_matrix)\n",
    "                else:\n",
    "                    adata2 = counts.values  # Keep it as dense numpy array\n",
    "\n",
    "                \"\"\"\n",
    "                [여기까지]\n",
    "                \"\"\"\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Step 1: Convert adata.X to float32 or int32 before creating the PyTorch tensor\n",
    "                if isinstance(adata.X, np.ndarray):\n",
    "                    # Convert to float32 if data is uint16 or other unsupported types\n",
    "                    adata_X = adata.X.astype(np.float32) if adata.X.dtype == np.uint16 else adata.X.astype(np.float32)\n",
    "                    adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                elif isinstance(adata.X, csr_matrix):\n",
    "                    adata_X = adata.X.toarray().astype(np.float32)\n",
    "                    adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    print('adata.X 데이터 형식 확인해주세요')\n",
    "                    \n",
    "                # Step 2: Convert adata2 to float32 or int32 before creating the PyTorch tensor\n",
    "                if isinstance(adata2, np.ndarray):\n",
    "                    # Convert to float32 if data is uint16 or other unsupported types\n",
    "                    adata2 = adata2.astype(np.float32) if adata2.dtype == np.uint16 else adata2.astype(np.float32)\n",
    "                    adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                elif isinstance(adata2, csr_matrix):\n",
    "                    adata2 = adata2.toarray().astype(np.float32)\n",
    "                    adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    print('adata2 데이터 형식 확인해주세요')\n",
    "            \n",
    "                # Step 3: Get the indices of the dropped values (where the mask is True)\n",
    "                dropped_indices = dropped_mask.nonzero(as_tuple=True)\n",
    "                \n",
    "                # Step 4: Extract the corresponding values for dropped spots\n",
    "                adata_values = adata_tensor[dropped_indices]\n",
    "                adata2_values = adata2_tensor[dropped_indices]\n",
    "                \n",
    "                # Step 5: Convert to CPU and calculate manh.\n",
    "                adata_values_cpu = adata_values.cpu().numpy()\n",
    "                adata2_values_cpu = adata2_values.cpu().numpy()\n",
    "                #print(adata_values_cpu[:10])\n",
    "                #print(adata2_values_cpu[:10])\n",
    "                     \n",
    "                manhattan_distance = np.abs(adata_values_cpu - adata2_values_cpu).sum()\n",
    "                num_values = adata_values_cpu.size\n",
    "                manhattan_score = manhattan_distance / num_values\n",
    "                \n",
    "                # Store the result in the dictionary\n",
    "                man_sc[dataset + \"/\" + adata_name] = manhattan_score\n",
    "                \n",
    "                print(man_sc)\n",
    "\n",
    "            row_name = \"Trial : \" + trial\n",
    "            my_dir = os.path.join('/home/s2022310812/bench_results', model,dataset)\n",
    "            csv_path = os.path.join(my_dir, percent+\"-\"+sample +\"_result.csv\")\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(my_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract column names and values\n",
    "            columns = [key.split(\"/\")[-1].split(\".\")[0] for key in man_sc.keys()]\n",
    "            values = list(man_sc.values())\n",
    "            \n",
    "            # Create a DataFrame for the new row\n",
    "            data = pd.DataFrame([values], columns=columns, index=[row_name])\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.exists(csv_path):\n",
    "                # Read the existing CSV\n",
    "                existing_data = pd.read_csv(csv_path, index_col=0)\n",
    "            \n",
    "                # Append the new row\n",
    "                updated_data = pd.concat([existing_data, data])\n",
    "            else:\n",
    "                # If the file doesn't exist, use the new DataFrame\n",
    "                updated_data = data\n",
    "            \n",
    "            # Save back to CSV\n",
    "            updated_data.to_csv(csv_path) # [수정] 파라미터 최적화 등 결과 저장하지 않고 돌려보는 단계면 이 줄 주석처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
