{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "마지막 수정: 2024.12.14 20:33\n",
    "\n",
    "[주의] \n",
    "!!!! 아직 dropout_data 만드는 중! 어디까지 만들었는지 확인하고 코드 돌리세요 존재 안하는 폴더 있을수도 !!!!\n",
    "!!!!!dropout_data의 X는 sparse array, int32으로 통일했습니다.!!!!!!\n",
    "!!!! adata 모두 normalize/log 했습니다. 전에 없던 오류가 생겼으면 얘가 원인일수도... !!!\n",
    "봐서 알겠지만 원본/dropout 파일들의 이름이 같습니다. 코딩하기 편하려고 통일했으니\n",
    "아무쪼록 헷갈리지 마시길...\n",
    "dropout_data에 layers/dropped가 추가되어있습니다. X랑 같은 모양으로 boolean \n",
    "형식이고, True인 데이터들이 drop 후 0으로 채운 위치입니다. 혹시 모델이 NaN을\n",
    "더 잘 impute한다면 이거 이용해서 데이터 수정하시면 됩니다.\n",
    "모델 별로 이 노트북을 새로 복제해서 만들어두면 나중에 배포용 코드 만들기 편하겠죠?\n",
    "이미지 파일이 필요하시면 hest_data/wsis 에서 가져오셔야 합니다. SPROD에 제가 만들어둔 가져오는\n",
    "코드가 있긴 하니 필요하시면 가져가시죠.\n",
    "이왕이면 dropout_data 폴더는 수정하지 말아주세요 (output 저장 포함)\n",
    "\n",
    "결과는 bench_results에 저장됩니다. 혹시 잘못 돌려서 데이터 삭제해야하면 수동으로 들어가서 수정하시죠 ㅎ\n",
    "\n",
    "[수정] 이라고 코멘트 해 둔 부분 적절히 고치시면 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패키지 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import torch\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "import cupy as cp\n",
    "\n",
    "#[수정] 이 밑으로 [여기까지] 지우고 여러분 모델에 맞는 패키지 불러오시면 됩니다.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "#[여기까지]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벤치마킹 데이터/환경 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_data_dir = '/home/s2022310812/bench_data' # bench_data 경로 [수정]\n",
    "dropout_data_dir = '/home/s2022310812/dropout_data' #dropout_data 경로 [수정]\n",
    "datasets = [ #[수정]\n",
    "    \"IDC\",\n",
    "  # \"COAD\",\n",
    "  # \"READ\",\n",
    "  # \"LYMPH_IDC\",\n",
    "]\n",
    "model = \"AverageSmoothing\" #[수정] 모델명\n",
    "percents = [ #[수정] dropout percentage\n",
    "    '10%',\n",
    "    '20%',\n",
    "    '30%'\n",
    "] \n",
    "samples = [ #[수정] dropout percentage data version\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "    '5'\n",
    "] \n",
    "\n",
    "trial = '2' #[수정] dropout percentage data version trial number (1-5)\n",
    "man_sc = {}\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu') #[수정] cuda:사용할 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벤치마킹 모델 구동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IDC/NCBI785.h5ad': 2.378798411062067, 'IDC/NCBI783.h5ad': 2.251506782175595, 'IDC/TENX95.h5ad': 2.200552784987382, 'IDC/TENX99.h5ad': 2.339003736807667}\n",
      "{'IDC/NCBI785.h5ad': 2.3821004322264985, 'IDC/NCBI783.h5ad': 2.2488680599766564, 'IDC/TENX95.h5ad': 2.205094764231314, 'IDC/TENX99.h5ad': 2.338804598697903}\n",
      "{'IDC/NCBI785.h5ad': 2.383773318190605, 'IDC/NCBI783.h5ad': 2.249933410889249, 'IDC/TENX95.h5ad': 2.205345590651442, 'IDC/TENX99.h5ad': 2.3432839804961385}\n",
      "{'IDC/NCBI785.h5ad': 2.3778285309108846, 'IDC/NCBI783.h5ad': 2.2574040520029692, 'IDC/TENX95.h5ad': 2.2036912020905923, 'IDC/TENX99.h5ad': 2.336824046268491}\n",
      "{'IDC/NCBI785.h5ad': 2.385367728090018, 'IDC/NCBI783.h5ad': 2.2491026000344947, 'IDC/TENX95.h5ad': 2.2055544876723565, 'IDC/TENX99.h5ad': 2.3401824096406987}\n",
      "{'IDC/NCBI785.h5ad': 2.2211118614120777, 'IDC/NCBI783.h5ad': 2.1882685817870278, 'IDC/TENX95.h5ad': 2.1683369644153956, 'IDC/TENX99.h5ad': 2.2773486891525567}\n",
      "{'IDC/NCBI785.h5ad': 2.2173722715355617, 'IDC/NCBI783.h5ad': 2.1827282751730186, 'IDC/TENX95.h5ad': 2.163738541240774, 'IDC/TENX99.h5ad': 2.2766064620574302}\n",
      "{'IDC/NCBI785.h5ad': 2.2237567849846003, 'IDC/NCBI783.h5ad': 2.179738250984156, 'IDC/TENX95.h5ad': 2.1639666684615797, 'IDC/TENX99.h5ad': 2.2796482952142636}\n",
      "{'IDC/NCBI785.h5ad': 2.2239327477573685, 'IDC/NCBI783.h5ad': 2.1853745356457135, 'IDC/TENX95.h5ad': 2.1654769022763753, 'IDC/TENX99.h5ad': 2.275028563017376}\n",
      "{'IDC/NCBI785.h5ad': 2.221726311457084, 'IDC/NCBI783.h5ad': 2.1769070450370207, 'IDC/TENX95.h5ad': 2.1638085609130493, 'IDC/TENX99.h5ad': 2.2786868256340793}\n",
      "{'IDC/NCBI785.h5ad': 2.1003187015318248, 'IDC/NCBI783.h5ad': 2.131214300471538, 'IDC/TENX95.h5ad': 2.1335912540500988, 'IDC/TENX99.h5ad': 2.2230000408279915}\n",
      "{'IDC/NCBI785.h5ad': 2.100361698541674, 'IDC/NCBI783.h5ad': 2.1298069086406057, 'IDC/TENX95.h5ad': 2.130101728470409, 'IDC/TENX99.h5ad': 2.225297238406518}\n",
      "{'IDC/NCBI785.h5ad': 2.0999029970921343, 'IDC/NCBI783.h5ad': 2.1296228620213262, 'IDC/TENX95.h5ad': 2.131449763306256, 'IDC/TENX99.h5ad': 2.2245875666637276}\n",
      "{'IDC/NCBI785.h5ad': 2.0995483099251935, 'IDC/NCBI783.h5ad': 2.135351201478743, 'IDC/TENX95.h5ad': 2.129424072979677, 'IDC/TENX99.h5ad': 2.224513712880342}\n",
      "{'IDC/NCBI785.h5ad': 2.0996587244365204, 'IDC/NCBI783.h5ad': 2.131238836135149, 'IDC/TENX95.h5ad': 2.132213721116859, 'IDC/TENX99.h5ad': 2.2242676227634846}\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for percent in percents:\n",
    "        for sample in samples:\n",
    "            bench_path = os.path.join(bench_data_dir,dataset,'adata')\n",
    "            dropout_path = os.path.join(dropout_data_dir,dataset,percent+\"-\"+sample)\n",
    "            \n",
    "            adata_names = [f for f in os.listdir(bench_path) if os.path.isfile(os.path.join(bench_path, f))]\n",
    "        \n",
    "            for adata_name in adata_names:\n",
    "\n",
    "                adata = ad.read_h5ad(os.path.join(bench_path,adata_name))\n",
    "                sc.pp.normalize_total(adata, target_sum=1e6, exclude_highly_expressed= False) \n",
    "                sc.pp.log1p(adata)\n",
    "                sc.pp.scale(adata)\n",
    "                \n",
    "                adata_drop = ad.read_h5ad(os.path.join(dropout_path,adata_name))\n",
    "                dropped_mask = torch.tensor(adata_drop.layers['dropped'], dtype=torch.bool, device=device)\n",
    "\n",
    "                sc.pp.normalize_total(adata_drop, target_sum=1e6, exclude_highly_expressed= False)\n",
    "                sc.pp.log1p(adata_drop)\n",
    "                sc.pp.scale(adata_drop)\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                [수정]\n",
    "                여기서부터 [여기까지] adata_drop.X에 본인 모델 적용시켜서 adata2에 저장해주시면 됩니다.\n",
    "                [주의] adata2에 \"\"adata.X 에 해당하는 파일\"\" 저장해주세요... 그냥 그렇게 만들었어요..\n",
    "                예시로 제 코드 보여드림. 여러분한테는 쓸모가 없겠죠? 지우고 시작하시면 됩니다 화이팅\n",
    "                \"\"\"\n",
    "\n",
    "                # Extract the matrix `X` from `adata2` and convert it to CuPy array\n",
    "                X = cp.array(adata_drop.X)\n",
    "                \n",
    "                # Create a padded version of the matrix to handle edge cases\n",
    "                padded_X = cp.pad(X, pad_width=1, mode='constant', constant_values=0)\n",
    "                \n",
    "                # Identify where X has 0 values\n",
    "                zero_mask = (X == 0)\n",
    "                \n",
    "                # Initialize the output matrix\n",
    "                smoothed_X = X.copy()\n",
    "                \n",
    "                # Iterate over neighbors to calculate the sum of the 9-neighbor values\n",
    "                neighbor_sum = cp.zeros_like(padded_X)\n",
    "                for dx, dy in [(-1, -1), (-1, 0), (-1, 1),\n",
    "                               (0, -1), (0, 0), (0, 1),\n",
    "                               (1, -1), (1, 0), (1, 1)]:\n",
    "                    neighbor_sum[1:-1, 1:-1] += padded_X[1+dx:X.shape[0]+1+dx, 1+dy:X.shape[1]+1+dy]\n",
    "                \n",
    "                # Remove padding from the neighbor_sum matrix\n",
    "                neighbor_sum = neighbor_sum[1:-1, 1:-1]\n",
    "                \n",
    "                # Calculate the average of the 9 neighbors\n",
    "                neighbor_count = 9  # Always 9 neighbors\n",
    "                neighbor_avg = neighbor_sum / neighbor_count\n",
    "                \n",
    "                # Replace 0 values with their smoothed averages\n",
    "                smoothed_X[zero_mask] = neighbor_avg[zero_mask]\n",
    "                \n",
    "                # Convert back to NumPy if needed\n",
    "                adata2 = cp.asnumpy(smoothed_X)\n",
    "\n",
    "                \"\"\"\n",
    "                [여기까지]\n",
    "                \"\"\"\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Step 1: Convert adata.X to float32 or int32 before creating the PyTorch tensor\n",
    "                if isinstance(adata.X, np.ndarray):\n",
    "                    # Convert to float32 if data is uint16 or other unsupported types\n",
    "                    adata_X = adata.X.astype(np.float32) if adata.X.dtype == np.uint16 else adata.X.astype(np.float32)\n",
    "                    adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                elif isinstance(adata.X, csr_matrix):\n",
    "                    adata_X = adata.X.toarray().astype(np.float32)\n",
    "                    adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    print('adata.X 데이터 형식 확인해주세요')\n",
    "                    \n",
    "                # Step 2: Convert adata2 to float32 or int32 before creating the PyTorch tensor\n",
    "                if isinstance(adata2, np.ndarray):\n",
    "                    # Convert to float32 if data is uint16 or other unsupported types\n",
    "                    adata2 = adata2.astype(np.float32) if adata2.dtype == np.uint16 else adata2.astype(np.float32)\n",
    "                    adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                elif isinstance(adata2, csr_matrix):\n",
    "                    adata2 = adata2.toarray().astype(np.float32)\n",
    "                    adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    print('adata2 데이터 형식 확인해주세요')\n",
    "            \n",
    "                # Step 3: Get the indices of the dropped values (where the mask is True)\n",
    "                dropped_indices = dropped_mask.nonzero(as_tuple=True)\n",
    "                \n",
    "                # Step 4: Extract the corresponding values for dropped spots\n",
    "                adata_values = adata_tensor[dropped_indices]\n",
    "                adata2_values = adata2_tensor[dropped_indices]\n",
    "                \n",
    "                # Step 5: Convert to CPU and calculate manh.\n",
    "                adata_values_cpu = adata_values.cpu().numpy()\n",
    "                adata2_values_cpu = adata2_values.cpu().numpy()\n",
    "\n",
    "                manhattan_distance = np.abs(adata_values_cpu - adata2_values_cpu).sum()\n",
    "                num_values = adata_values_cpu.size\n",
    "                manhattan_score = manhattan_distance / num_values\n",
    "                \n",
    "                # Store the result in the dictionary\n",
    "                man_sc[dataset + \"/\" + adata_name] = manhattan_score\n",
    "                \n",
    "            print(man_sc)\n",
    "\n",
    "            row_name = \"Trial : \" + trial\n",
    "            my_dir = os.path.join('/home/s2022310812/bench_results', model,dataset)\n",
    "            csv_path = os.path.join(my_dir, percent+\"-\"+sample +\"_result.csv\")\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(my_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract column names and values\n",
    "            columns = [key.split(\"/\")[-1].split(\".\")[0] for key in man_sc.keys()]\n",
    "            values = list(man_sc.values())\n",
    "            \n",
    "            # Create a DataFrame for the new row\n",
    "            data = pd.DataFrame([values], columns=columns, index=[row_name])\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.exists(csv_path):\n",
    "                # Read the existing CSV\n",
    "                existing_data = pd.read_csv(csv_path, index_col=0)\n",
    "            \n",
    "                # Append the new row\n",
    "                updated_data = pd.concat([existing_data, data])\n",
    "            else:\n",
    "                # If the file doesn't exist, use the new DataFrame\n",
    "                updated_data = data\n",
    "            \n",
    "            # Save back to CSV\n",
    "            #updated_data.to_csv(csv_path) # [수정] 파라미터 최적화 등 결과 저장하지 않고 돌려보는 단계면 이 줄 주석처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
