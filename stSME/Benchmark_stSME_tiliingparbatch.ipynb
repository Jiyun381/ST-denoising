{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17d492-94ed-4bc7-af77-9e957641bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[주의] \n",
    "!!!! 아직 dropout_data 만드는 중! 어디까지 만들었는지 확인하고 코드 돌리세요 존재 안하는 폴더 있을수도 !!!!\n",
    "!!!!!dropout_data의 X는 sparse array, int32으로 통일했습니다.!!!!!!\n",
    "!!!! adata 모두 normalize/log 했습니다. 전에 없던 오류가 생겼으면 얘가 원인일수도... !!!\n",
    "봐서 알겠지만 원본/dropout 파일들의 이름이 같습니다. 코딩하기 편하려고 통일했으니\n",
    "아무쪼록 헷갈리지 마시길...\n",
    "dropout_data에 layers/dropped가 추가되어있습니다. X랑 같은 모양으로 boolean \n",
    "형식이고, True인 데이터들이 drop 후 0으로 채운 위치입니다. 혹시 모델이 NaN을\n",
    "더 잘 impute한다면 이거 이용해서 데이터 수정하시면 됩니다.\n",
    "모델 별로 이 노트북을 새로 복제해서 만들어두면 나중에 배포용 코드 만들기 편하겠죠?\n",
    "이미지 파일이 필요하시면 hest_data/wsis 에서 가져오셔야 합니다. SPROD에 제가 만들어둔 가져오는\n",
    "코드가 있긴 하니 필요하시면 가져가시죠.\n",
    "이왕이면 dropout_data 폴더는 수정하지 말아주세요 (output 저장 포함)\n",
    "\n",
    "결과는 bench_results에 저장됩니다. 혹시 잘못 돌려서 데이터 삭제해야하면 수동으로 들어가서 수정하시죠 ㅎ\n",
    "\n",
    "[수정] 이라고 코멘트 해 둔 부분 적절히 고치시면 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0bc32-23e3-4873-b56f-2ef27b93605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b07c0-866e-42aa-abd0-1bf6ae536c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best GPU for stlearn/TensorFlow with the priority: cuda:2, cuda:0, cuda:1\n",
    "# If none is available, print message and use CPU for stlearn.\n",
    "\n",
    "device_priority = [1,0, 2]  # Highest priority first\n",
    "\n",
    "# Get the list of available GPUs\n",
    "available_gpus = GPUtil.getAvailable(order='first', limit=len(device_priority), maxLoad=0.5, maxMemory=0.5, includeNan=False)\n",
    "\n",
    "# Select GPUs based on priority\n",
    "selected_tf_devices = []\n",
    "for d in device_priority:\n",
    "    if d in available_gpus:\n",
    "        selected_tf_devices.append(str(d))\n",
    "        # Uncomment the next line if you want to select only one GPU based on priority\n",
    "        # break\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES\n",
    "if not selected_tf_devices:\n",
    "    print(\"No suitable GPU found for stlearn/TensorFlow. Using CPU for stSME.\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(selected_tf_devices)\n",
    "    print(f\"Using GPU(s) {', '.join(selected_tf_devices)} for stlearn/TensorFlow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a703572d-6946-430d-bf67-88f3189d2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu') #[수정] cuda:사용할 GPU\n",
    "print(f\"Using device {device} for PyTorch operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593f39e-0619-4fd4-af66-4d03710e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d734ae-7e91-45e8-b2bd-e26e68410b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import stlearn as st\n",
    "from pathlib import Path\n",
    "from tifffile import TiffFile\n",
    "from parallel_tilingv2_batch import tiling_optimized\n",
    "from parallel_extract_feature import extract_feature_optimized\n",
    "import random\n",
    "\n",
    "st.settings.set_figure_params(dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cd16e-39e3-42ac-b7f8-66dcdc2d9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_data_dir = '/home/s2022310812/bench_data' # bench_data 경로 [수정]\n",
    "dropout_data_dir = '/home/s2022310812/dropout_data' #dropout_data 경로 [수정]\n",
    "datasets = [ #[수정]\n",
    "    \"IDC\",\n",
    "  # \"COAD\",\n",
    "  # \"READ\",\n",
    "  # \"LYMPH_IDC\",\n",
    "]\n",
    "model = \"stSME\" #[수정] 모델명\n",
    "percents = [ #[수정] dropout percentage\n",
    "    '30%',\n",
    "    #'20%',\n",
    "    #'30%'\n",
    "] \n",
    "samples = [ #[수정] dropout percentage data version\n",
    "    '1',\n",
    "    #'2',\n",
    "    #'3',\n",
    "    #'4',\n",
    "    #'5'\n",
    "] \n",
    "\n",
    "trial_range = range(1, 6)  # 5 trials\n",
    "\n",
    "man_sc = {}\n",
    "\n",
    "# Option to save results to CSV or just print them out\n",
    "save_results = True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd24e2-9b3e-49b6-a039-5f09101f4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image directory for tiling\n",
    "wsis_dir = '/home/s2022310812/hest_data/wsis'  # The directory where WSIs are stored\n",
    "\n",
    "TILE_PATH = Path(\"/home/s2022310812/stlearn/stSME/bench/v1/tmp/tiles\")\n",
    "TILE_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d190e2-88ce-4807-936b-dfaf7317b1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for trial in trial_range[:1]:\n",
    "    seed = trial\n",
    "    print(f\"\\n===== Starting Trial {trial} with seed {seed} =====\")\n",
    "    \n",
    "    # -------------------- Set Random Seeds --------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Set TensorFlow's random seed\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    print(f\"Random seed set to {seed} for Trial {trial}.\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for percent in percents:\n",
    "            for sample in samples:\n",
    "                bench_path = os.path.join(bench_data_dir,dataset,'adata')\n",
    "                dropout_path = os.path.join(dropout_data_dir,dataset,percent+\"-\"+sample)\n",
    "                \n",
    "                adata_names = [f for f in os.listdir(bench_path) if os.path.isfile(os.path.join(bench_path, f))]\n",
    "            \n",
    "                for adata_name in adata_names[:3]:\n",
    "    \n",
    "                    adata = ad.read_h5ad(os.path.join(bench_path,adata_name))\n",
    "                    sc.pp.normalize_total(adata, target_sum=1e6, exclude_highly_expressed= False) \n",
    "                    sc.pp.log1p(adata)\n",
    "                    sc.pp.scale(adata)\n",
    "                    \n",
    "                    adata_drop = ad.read_h5ad(os.path.join(dropout_path,adata_name))\n",
    "                    sc.pp.normalize_total(adata_drop, target_sum=1e6, exclude_highly_expressed= False)\n",
    "                    sc.pp.log1p(adata_drop)\n",
    "                    sc.pp.scale(adata_drop)\n",
    "                    dropped_mask = torch.tensor(adata_drop.layers['dropped'], dtype=torch.bool, device=device)\n",
    "    \n",
    "                    \"\"\"\n",
    "                    [수정]\n",
    "                    여기서부터 [여기까지] adata_drop.X에 본인 모델 적용시켜서 adata2에 저장해주시면 됩니다.\n",
    "                    [주의] adata2에 \"\"adata.X 에 해당하는 파일\"\" 저장해주세요... 그냥 그렇게 만들었어요..\n",
    "                    예시로 제 코드 보여드림. 여러분한테는 쓸모가 없겠죠? 지우고 시작하시면 됩니다 화이팅\n",
    "                    \"\"\"\n",
    "                   \n",
    "                    # Derive base_name from adata_name (remove .h5ad extension)\n",
    "                    base_name = adata_name.replace(\".h5ad\", \"\")\n",
    "    \n",
    "                    # Use base_name as sample_id for clarity (optional but recommended)\n",
    "                    sample_id = base_name\n",
    "    \n",
    "                    # Load the corresponding .tif image\n",
    "                    tif_file = os.path.join(wsis_dir, f\"{base_name}.tif\")\n",
    "    \n",
    "                    if os.path.exists(tif_file):\n",
    "                        # Load the .tif file and extract the highest resolution image\n",
    "                        with TiffFile(tif_file) as tif:\n",
    "                            highest_res_image = tif.pages[0].asarray()\n",
    "    \n",
    "                        # Check if the image is not all zeros\n",
    "                        if not np.any(highest_res_image):\n",
    "                            print(f\"Warning: The loaded image {tif_file} contains only zeros.\")\n",
    "                        else:\n",
    "                            print(f\"Loaded image {tif_file} with shape {highest_res_image.shape}.\")\n",
    "                            \n",
    "                        # Normalize image [0,255] -> [0,1] if needed\n",
    "                        highest_res_image = highest_res_image.astype(float) / 255.0\n",
    "    \n",
    "                        # Construct the spatial dictionary under adata_drop.uns\n",
    "                        adata_drop.uns[\"spatial\"] = {\n",
    "                            sample_id: {\n",
    "                                \"images\": {\n",
    "                                    \"hires\": highest_res_image\n",
    "                                },\n",
    "                                \"scalefactors\": {\n",
    "                                    \"tissue_hires_scalef\": 1\n",
    "                                },\n",
    "                                \"use_quality\": \"hires\"\n",
    "                            }\n",
    "                        }\n",
    "    \n",
    "                        # Ensure imagerow and imagecol columns in adata_drop.obs\n",
    "                        # Assuming adata_drop.obsm[\"spatial\"] is Nx2 with [x,y] in pixel coordinates.\n",
    "                        adata_drop.obs[\"imagerow\"] = adata_drop.obsm[\"spatial\"][:, 1]  # y-coordinate\n",
    "                        adata_drop.obs[\"imagecol\"] = adata_drop.obsm[\"spatial\"][:, 0]  # x-coordinate\n",
    "    \n",
    "                        # Check the structure\n",
    "                        print(\"adata_drop.uns['spatial'] structure:\")\n",
    "                        print(adata_drop.uns[\"spatial\"])\n",
    "                    else:\n",
    "                        print(f\"TIF file {tif_file} does not exist. Please check your wsis_dir or file name.\")\n",
    "    \n",
    "                    # Apply stSME\n",
    "                    tiling_optimized(adata_drop, out_path=TILE_PATH, crop_size=40, target_size=299, num_workers=40, verbose=True)\n",
    "                    #st.pp.tiling(adata_drop, out_path=TILE_PATH, crop_size=40, target_size=299)\n",
    "                    #extract_feature_optimized(adata_drop, cnn_base=\"resnet50\", n_components=50, seeds=seed)\n",
    "                    st.pp.extract_feature(adata_drop, cnn_base=\"resnet50\", n_components=50, seeds=seed)\n",
    "    \n",
    "                    # Run PCA before SME\n",
    "                    st.em.run_pca(adata_drop, n_comps=50)\n",
    "    \n",
    "                    # SME normalization using data already normalized/log-transformed:\n",
    "                    st.spatial.SME.SME_normalize(adata_drop, use_data=\"raw\")\n",
    "                    \n",
    "                    # After normalization, SME results are in adata_drop.obsm['raw_SME_normalized']\n",
    "                    # After normalization, SME results are in adata_drop.obsm['raw_SME_normalized']\n",
    "                    if \"raw_SME_normalized\" not in adata_drop.obsm:\n",
    "                        print(f\"Error: SME normalization did not add 'raw_SME_normalized' to adata.obsm for {adata_name}.\")\n",
    "                        continue\n",
    "    \n",
    "                    # Save SME results to adata2\n",
    "                    adata2 = adata_drop.obsm['raw_SME_normalized']\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    [여기까지]\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # Step 1: Convert adata.X to float32 or int32 before creating the PyTorch tensor\n",
    "                    if isinstance(adata.X, np.ndarray):\n",
    "                        # Convert to float32 if data is uint16 or other unsupported types\n",
    "                        adata_X = adata.X.astype(np.float32) if adata.X.dtype == np.uint16 else adata.X.astype(np.float32)\n",
    "                        adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                    elif isinstance(adata.X, csr_matrix):\n",
    "                        adata_X = adata.X.toarray().astype(np.float32)\n",
    "                        adata_tensor = torch.tensor(adata_X, dtype=torch.float32, device=device)\n",
    "                    else:\n",
    "                        print('adata.X 데이터 형식 확인해주세요')\n",
    "                        \n",
    "                    # Step 2: Convert adata2 to float32 or int32 before creating the PyTorch tensor\n",
    "                    if isinstance(adata2, np.ndarray):\n",
    "                        # Convert to float32 if data is uint16 or other unsupported types\n",
    "                        adata2 = adata2.astype(np.float32) if adata2.dtype == np.uint16 else adata2.astype(np.float32)\n",
    "                        adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                    elif isinstance(adata2, csr_matrix):\n",
    "                        adata2 = adata2.toarray().astype(np.float32)\n",
    "                        adata2_tensor = torch.tensor(adata2, dtype=torch.float32, device=device)\n",
    "                    else:\n",
    "                        print('adata2 데이터 형식 확인해주세요')\n",
    "                \n",
    "                    # Step 3: Get the indices of the dropped values (where the mask is True)\n",
    "                    dropped_indices = dropped_mask.nonzero(as_tuple=True)\n",
    "                    \n",
    "                    # Step 4: Extract the corresponding values for dropped spots\n",
    "                    adata_values = adata_tensor[dropped_indices]\n",
    "                    adata2_values = adata2_tensor[dropped_indices]\n",
    "                    \n",
    "                    # Step 5: Convert to CPU and calculate manh.\n",
    "                    adata_values_cpu = adata_values.cpu().numpy()\n",
    "                    adata2_values_cpu = adata2_values.cpu().numpy()\n",
    "                    #print(adata_values_cpu[:10])\n",
    "                    #print(adata2_values_cpu[:10])\n",
    "                         \n",
    "                    manhattan_distance = np.abs(adata_values_cpu - adata2_values_cpu).sum()\n",
    "                    num_values = adata_values_cpu.size\n",
    "                    manhattan_score = manhattan_distance / num_values\n",
    "                    \n",
    "                    # Store the result in the dictionary\n",
    "                    man_sc[dataset + \"/\" + adata_name] = manhattan_score\n",
    "                    \n",
    "                    print(f\"Processed {adata_name}, Manhattan score: {manhattan_score}\")\n",
    "\n",
    "                # After processing all adata_names\n",
    "                if not man_sc:\n",
    "                    print(f\"No results for {dataset}/{percent}-{sample} in Trial {trial}, skipping CSV write.\")\n",
    "                    continue\n",
    "    \n",
    "                row_name = f\"Trial : {trial}\"\n",
    "                my_dir = os.path.join('/home/s2022310812/bench_results', model,dataset)\n",
    "                csv_path = os.path.join(my_dir, percent+\"-\"+sample +\"_result.csv\")\n",
    "                # Ensure the directory exists\n",
    "                os.makedirs(my_dir, exist_ok=True)\n",
    "                \n",
    "                # Extract column names and values\n",
    "                columns = [key.split(\"/\")[-1].split(\".\")[0] for key in man_sc.keys()]\n",
    "                values = list(man_sc.values())\n",
    "                \n",
    "                # Create a DataFrame for the new row\n",
    "                data = pd.DataFrame([values], columns=columns, index=[row_name])\n",
    "\n",
    "                 # Only save if save_results = True\n",
    "                if save_results:\n",
    "                    # Attempt to read existing data\n",
    "                    existing_data = None\n",
    "                    if os.path.exists(csv_path):\n",
    "                        print(f\"CSV file '{csv_path}' exists. Attempting to read it.\")\n",
    "                        try:\n",
    "                            existing_data = pd.read_csv(csv_path, index_col=0)\n",
    "                            print(f\"Successfully read existing CSV with shape {existing_data.shape}.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Error reading existing CSV '{csv_path}': {e}\")\n",
    "                            print(\"A new CSV will be created.\")\n",
    "                            existing_data = None\n",
    "                    else:\n",
    "                        print(f\"CSV file '{csv_path}' does not exist. A new one will be created.\")\n",
    "\n",
    "                    if existing_data is not None:\n",
    "                        updated_data = pd.concat([existing_data, data])\n",
    "                    else:\n",
    "                        updated_data = data\n",
    "\n",
    "                    # Try saving the CSV\n",
    "                    print(f\"Attempting to save results to '{csv_path}'...\")\n",
    "                    try:\n",
    "                        updated_data.to_csv(csv_path)\n",
    "                        print(f\"Saved results to '{csv_path}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving CSV '{csv_path}': {e}\")\n",
    "                else:\n",
    "                    print(\"save_results=False, not saving results to CSV. Results for this trial:\")\n",
    "                    print(data)\n",
    "\n",
    "    print(f\"===== Completed Trial {trial} =====\\n\")\n",
    "\n",
    "print(\"Benchmarking completed.\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfe107-9d88-4918-ae5b-8a3c8b225a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d8fb2-23c7-4d51-abf9-d5c3e381d8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (stlearn_v2)",
   "language": "python",
   "name": "ip_stlearnv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
